{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# if using mac\n",
    "df = pd.read_excel('/Users/zhanghantang/PycharmProjects/sucide_project/BIOM40forUSC.xlsx')\n",
    "data = df[df['SI'].notnull()]\n",
    "y = data['SI']\n",
    "x = data.loc[:, 'GIMAP1Biom1552316_a_at':'CFIS']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\PHD\\\\sucide_project\\\\BIOM40forUSC.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/73/3f_t97jx5y7d14c8_np58h8m0000gn/T/ipykernel_56937/2421970234.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# windows\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_excel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'E:\\PHD\\sucide_project\\BIOM40forUSC.xlsx'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'SI'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotnull\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'SI'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'GIMAP1Biom1552316_a_at'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m'CFIS'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001B[0m in \u001B[0;36mread_excel\u001B[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[1;32m    362\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    363\u001B[0m         \u001B[0mshould_close\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 364\u001B[0;31m         \u001B[0mio\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    365\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    366\u001B[0m         raise ValueError(\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[1;32m   1189\u001B[0m                 \u001B[0mext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"xls\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1190\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1191\u001B[0;31m                 ext = inspect_excel_format(\n\u001B[0m\u001B[1;32m   1192\u001B[0m                     \u001B[0mcontent_or_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1193\u001B[0m                 )\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001B[0m in \u001B[0;36minspect_excel_format\u001B[0;34m(content_or_path, storage_options)\u001B[0m\n\u001B[1;32m   1068\u001B[0m         \u001B[0mcontent_or_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBytesIO\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcontent_or_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1069\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1070\u001B[0;31m     with get_handle(\n\u001B[0m\u001B[1;32m   1071\u001B[0m         \u001B[0mcontent_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_text\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1072\u001B[0m     ) as handle:\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    709\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    710\u001B[0m             \u001B[0;31m# Binary mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 711\u001B[0;31m             \u001B[0mhandle\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    712\u001B[0m         \u001B[0mhandles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    713\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'E:\\\\PHD\\\\sucide_project\\\\BIOM40forUSC.xlsx'"
     ]
    }
   ],
   "source": [
    "# windows\n",
    "df = pd.read_excel('E:\\PHD\\sucide_project\\BIOM40forUSC.xlsx')\n",
    "data = df[df['SI'].notnull()]\n",
    "y = data['SI']\n",
    "x = data.loc[:, 'GIMAP1Biom1552316_a_at':'CFIS']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603 151\n"
     ]
    }
   ],
   "source": [
    "# column 'CFI-S.PheneVisit' data type is string, i am not clear its internal meaning and how to convert to float data type\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = x.drop(labels='CFI-S.PheneVisit', axis=1)\n",
    "# drop these column directly\n",
    "x = data.loc[:, 'GIMAP1Biom1552316_a_at':'RAB3GAP2Biom240234_at']\n",
    "new_y = []\n",
    "for i in y:\n",
    "    if i == 0 or i ==1:\n",
    "        new_y.append(0)\n",
    "    else:\n",
    "        new_y.append(1)\n",
    "y = np.array(new_y)\n",
    "X = StandardScaler().fit_transform(x)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.2, random_state=10)\n",
    "print(len(X_train), len(X_test))\n",
    "\n",
    "# todo unified randome state\n",
    "unified_random_state = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about logistic regression content\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty:  l1\n",
      "Best C:  100\n",
      "Best score:  0.19299478970021897\n"
     ]
    }
   ],
   "source": [
    "# tune parameter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = LR(solver='saga')\n",
    "# 不同的solver可能只支持不同的有限的penalty\n",
    "param_grid = {'penalty':['l1', 'l2'], 'C':[0.1, 1, 10, 100], 'max_iter':[3000, 3500]}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', return_train_score=True)\n",
    "grid_search.fit(X, new_y)\n",
    "print(\"Best penalty: \", grid_search.best_params_['penalty'])\n",
    "print(\"Best C: \", grid_search.best_params_['C'])\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "'''\n",
    "Best penalty:  l1\n",
    "Best C:  100\n",
    "Best score:  0.19299478970021897\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.8013245  0.76821192 0.74834437 0.76821192 0.73333333], mean accuary is 0.7638852097130242\n",
      "Scores: precision is [0.44444444 0.25       0.09090909 0.23076923 0.35      ], mean precision is 0.2732245532245532\n",
      "Scores: recall is [0.13793103 0.10344828 0.03448276 0.10714286 0.5       ], mean recall is 0.17660098522167486\n",
      "Scores: f1 is [0.21052632 0.14634146 0.05       0.14634146 0.41176471], mean f1 is 0.19299478970021897\n",
      "Scores: roc_auc is [0.5607688  0.63793103 0.58790277 0.67799071 0.7192623 ], mean roc_auc is 0.6367711207799311\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nScore with L1 penalty: accuracy:0.7831325301204819 precision:0.4074074074074074 recall:0.22448979591836735 f1:0.2894736842105263 roc_auc:0.5722448979591838\\n'"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "lr_model = LR(solver='saga', penalty='l1', C=100, max_iter=3000)\n",
    "scores = cross_validate(lr_model, X, y, cv=5, scoring=scoring)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: precision is {}, mean precision is {}\".format(scores['test_precision'], scores['test_precision'].mean()))\n",
    "print(\"Scores: recall is {}, mean recall is {}\".format(scores['test_recall'], scores['test_recall'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1'], scores['test_f1'].mean()))\n",
    "print(\"Scores: roc_auc is {}, mean roc_auc is {}\".format(scores['test_roc_auc'], scores['test_roc_auc'].mean()))\n",
    "\n",
    "\n",
    "'''\n",
    "Scores: accuracy is [0.8013245  0.76821192 0.74834437 0.76821192 0.73333333], mean accuary is 0.7638852097130242\n",
    "Scores: precision is [0.44444444 0.25       0.09090909 0.23076923 0.35      ], mean precision is 0.2732245532245532\n",
    "Scores: recall is [0.13793103 0.10344828 0.03448276 0.10714286 0.5       ], mean recall is 0.17660098522167486\n",
    "Scores: f1 is [0.21052632 0.14634146 0.05       0.14634146 0.41176471], mean f1 is 0.19299478970021897\n",
    "Scores: roc_auc is [0.5607688  0.63793103 0.58790277 0.67799071 0.7192623 ], mean roc_auc is 0.6367711207799311\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about SVM\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  100\n",
      "Best score:  0.26710847637761254\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nBest penalty:  rbf\\nBest C:  10\\nScores:  [0.41666667 0.3902439  0.45454545 0.30769231 0.38596491]\\nMean Score:  0.391022648724831\\n'"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = SVC(kernel='rbf', max_iter=10000)\n",
    "param_grid = {'C':[1, 10, 100], }\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', return_train_score=True)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print(\"Best penalty: \", grid_search.best_params_['kernel'])\n",
    "print(\"Best C: \", grid_search.best_params_['C'])\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "'''\n",
    "Best penalty:  rbf\n",
    "Best C:  100\n",
    "\n",
    "Scores:  [0.36065574 0.44067797 0.45901639 0.31111111 0.40740741]\n",
    "Mean Score:  0.39577372315355086\n",
    "Scores:  [0.74274194 0.71833931 0.72649573 0.70439633 0.69226137]\n",
    "Mean Score:  0.7168469323092028\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about SVM\n",
    "-------------------------\n",
    "'''\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = SVC(C=100, kernel='rbf', max_iter=10000)\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=scoring)\n",
    "# y_pred = model.predict(X)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: precision is {}, mean accuary is {}\".format(scores['test_precision'], scores['test_precision'].mean()))\n",
    "print(\"Scores: recall is {}, mean accuary is {}\".format(scores['test_recall'], scores['test_recall'].mean()))\n",
    "print(\"Scores: f1 is {}, mean accuary is {}\".format(scores['test_f1'], scores['test_f1'].mean()))\n",
    "print(\"Scores: roc_auc is {}, mean accuary is {}\".format(scores['test_roc_auc'], scores['test_roc_auc'].mean()))\n",
    "\n",
    "'''\n",
    "Scores: accuracy is [0.7615894  0.74834437 0.74172185 0.7615894  0.58      ], mean accuary is 0.7186490066225166\n",
    "Scores: precision is [0.29411765 0.15384615 0.33333333 0.33333333 0.26027397], mean accuary is 0.27498088803487675\n",
    "Scores: recall is [0.17241379 0.06896552 0.34482759 0.28571429 0.67857143], mean accuary is 0.31009852216748773\n",
    "Scores: f1 is [0.2173913  0.0952381  0.33898305 0.30769231 0.37623762], mean accuary is 0.26710847637761254\n",
    "Scores: roc_auc is [0.56783493 0.60655738 0.65008479 0.71544715 0.69818501], mean accuary is 0.6476218543781174\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about random forest\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 11, 'min_samples_leaf': 2, 'min_samples_split': 6, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "param_grid = {'n_estimators':[10, 100, 200], 'max_depth':[8, 11, 13], 'min_samples_split': [4, 6, 8, 10],\n",
    "              'min_samples_leaf': [2, 4, 6]}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "rf_model = RandomForestClassifier(random_state=unified_random_state)\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=skf, scoring='roc_auc')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.8013245  0.81456954 0.82119205 0.82119205 0.76      ], mean accuary is 0.8036556291390727\n",
      "Scores: precision is [0.         1.         1.         1.         0.38235294], mean precision is 0.6764705882352942\n",
      "Scores: recall is [0.         0.03448276 0.06896552 0.03571429 0.46428571], mean recall is 0.12068965517241378\n",
      "Scores: f1 is [0.         0.06666667 0.12903226 0.06896552 0.41935484], mean f1 is 0.1368038561364479\n",
      "Scores: roc_auc is [0.49590164 0.51724138 0.53448276 0.51785714 0.64607728], mean roc_auc is 0.542312040700961\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n200， 11， 2， 6\\nScores: accuracy is [0.80794702 0.81456954 0.82119205 0.82119205 0.76666667], mean accuary is 0.8063134657836644\\nScores: precision is [0.         1.         1.         1.         0.39393939], mean precision is 0.6787878787878788\\nScores: recall is [0.         0.03448276 0.06896552 0.03571429 0.46428571], mean recall is 0.12068965517241378\\nScores: f1 is [0.         0.06666667 0.12903226 0.06896552 0.42622951], mean f1 is 0.13817879003385666\\nScores: roc_auc is [0.5        0.51724138 0.53448276 0.51785714 0.65017564], mean roc_auc is 0.543951384963256\\n\\n'"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision':make_scorer(precision_score, zero_division=0),\n",
    "    'recall' :make_scorer(recall_score),\n",
    "    'f1':make_scorer(f1_score),\n",
    "    'roc_auc':make_scorer(roc_auc_score)\n",
    "}\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=11, min_samples_leaf=2, min_samples_split=6, random_state=unified_random_state)\n",
    "\n",
    "# scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "scores = cross_validate(rf_model, X, y, cv=skf, scoring=scoring)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: precision is {}, mean precision is {}\".format(scores['test_precision'], scores['test_precision'].mean()))\n",
    "print(\"Scores: recall is {}, mean recall is {}\".format(scores['test_recall'], scores['test_recall'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1'], scores['test_f1'].mean()))\n",
    "print(\"Scores: roc_auc is {}, mean roc_auc is {}\".format(scores['test_roc_auc'], scores['test_roc_auc'].mean()))\n",
    "\n",
    "# print(\"Score with L1 penalty: accuracy:{} precision:{} recall:{} f1:{} roc_auc:{}\".format(accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred), roc_auc_score(y_test, y_pred)))\n",
    "'''\n",
    "200， 11， 2， 6\n",
    "Scores: accuracy is [0.80794702 0.81456954 0.82119205 0.82119205 0.76666667], mean accuary is 0.8063134657836644\n",
    "Scores: precision is [0.         1.         1.         1.         0.39393939], mean precision is 0.6787878787878788\n",
    "Scores: recall is [0.         0.03448276 0.06896552 0.03571429 0.46428571], mean recall is 0.12068965517241378\n",
    "Scores: f1 is [0.         0.06666667 0.12903226 0.06896552 0.42622951], mean f1 is 0.13817879003385666\n",
    "Scores: roc_auc is [0.5        0.51724138 0.53448276 0.51785714 0.65017564], mean roc_auc is 0.543951384963256\n",
    "\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about xgboost\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "clf = XGBClassifier(random_state=unified_random_state)\n",
    "param_grid = {'n_estimators':[10, 100, 200], 'max_depth':[4, 6, 8], 'learning_rate': [0.3, 0.01]}\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=skf, scoring='roc_auc')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.82119205 0.78145695 0.80794702 0.79470199 0.71333333], mean accuary is 0.7837262693156732\n",
      "Scores: precision is [0.66666667 0.16666667 0.5        0.36363636 0.34042553], mean precision is 0.40747904577691807\n",
      "Scores: recall is [0.13793103 0.03448276 0.13793103 0.14285714 0.57142857], mean recall is 0.20492610837438424\n",
      "Scores: f1 is [0.22857143 0.05714286 0.21621622 0.20512821 0.42666667], mean f1 is 0.22674507474507472\n",
      "Scores: roc_auc is [0.5607688  0.49674958 0.55257207 0.54297329 0.65866511], mean roc_auc is 0.5623457677684263\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nn_estimators=250, learning_rate=0.01, max_depth=4, subsample=0.9,\\n                    colsample_bytree=0.8,\\nSScores: accuracy is [0.82781457 0.79470199 0.81456954 0.8013245  0.75333333], mean accuary is 0.7983487858719647\\nScores: precision is [0.8        0.33333333 0.54545455 0.4        0.39534884], mean precision is 0.49482734319943616\\nScores: recall is [0.13793103 0.06896552 0.20689655 0.14285714 0.60714286], mean recall is 0.23275862068965517\\nScores: f1 is [0.23529412 0.11428571 0.3        0.21052632 0.47887324], mean f1 is 0.2677958774317733\\nScores: roc_auc is [0.56486716 0.51808932 0.58295647 0.54703833 0.69701405], mean roc_auc is 0.5819930648430282\\n'"
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision':make_scorer(precision_score, zero_division=0),\n",
    "    'recall' :make_scorer(recall_score),\n",
    "    'f1':make_scorer(f1_score),\n",
    "    'roc_auc':make_scorer(roc_auc_score)\n",
    "}\n",
    "clf = XGBClassifier(n_estimators=250, learning_rate=0.07, max_depth=4, subsample=0.9,\n",
    "                    colsample_bytree=0.8, reg_alpha=0.5)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "scores = cross_validate(clf, X, y, cv=skf, scoring=scoring)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: precision is {}, mean precision is {}\".format(scores['test_precision'], scores['test_precision'].mean()))\n",
    "print(\"Scores: recall is {}, mean recall is {}\".format(scores['test_recall'], scores['test_recall'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1'], scores['test_f1'].mean()))\n",
    "print(\"Scores: roc_auc is {}, mean roc_auc is {}\".format(scores['test_roc_auc'], scores['test_roc_auc'].mean()))\n",
    "\n",
    "'''\n",
    "n_estimators=250, learning_rate=0.07, max_depth=4, subsample=0.9,\n",
    "                    colsample_bytree=0.8, reg_alpha=0.5\n",
    "Scores: accuracy is [0.82781457 0.79470199 0.81456954 0.8013245  0.75333333], mean accuary is 0.7983487858719647\n",
    "Scores: precision is [0.8        0.33333333 0.54545455 0.4        0.39534884], mean precision is 0.49482734319943616\n",
    "Scores: recall is [0.13793103 0.06896552 0.20689655 0.14285714 0.60714286], mean recall is 0.23275862068965517\n",
    "Scores: f1 is [0.23529412 0.11428571 0.3        0.21052632 0.47887324], mean f1 is 0.2677958774317733\n",
    "Scores: roc_auc is [0.56486716 0.51808932 0.58295647 0.54703833 0.69701405], mean roc_auc is 0.5819930648430282\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about native bayes\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "nb = GaussianNB()\n",
    "# gaussian nb does not have many tunable hyparameters\n",
    "param_grid = {'n_estimators':[10, 100, 200], 'max_depth':[4, 6, 8], 'learning_rate': [0.3, 0.01]}\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=skf, scoring='roc_auc')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.56291391 0.54966887 0.37086093 0.2384106  0.38      ], mean accuary is 0.42037086092715226\n",
      "Scores: precision is [0.1509434  0.24675325 0.17       0.16793893 0.20183486], mean precision is 0.18749408733253856\n",
      "Scores: recall is [0.27586207 0.65517241 0.5862069  0.78571429 0.78571429], mean recall is 0.6177339901477832\n",
      "Scores: f1 is [0.19512195 0.35849057 0.26356589 0.27672956 0.32116788], mean f1 is 0.2830151703380446\n",
      "Scores: roc_auc is [0.4535048  0.58988129 0.45293951 0.44976771 0.53629977], mean roc_auc is 0.4964786170917511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "nb = GaussianNB()\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision':make_scorer(precision_score, zero_division=0),\n",
    "    'recall' :make_scorer(recall_score),\n",
    "    'f1':make_scorer(f1_score),\n",
    "    'roc_auc':make_scorer(roc_auc_score)\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "scores = cross_validate(nb, X, y, cv=skf, scoring=scoring)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: precision is {}, mean precision is {}\".format(scores['test_precision'], scores['test_precision'].mean()))\n",
    "print(\"Scores: recall is {}, mean recall is {}\".format(scores['test_recall'], scores['test_recall'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1'], scores['test_f1'].mean()))\n",
    "print(\"Scores: roc_auc is {}, mean roc_auc is {}\".format(scores['test_roc_auc'], scores['test_roc_auc'].mean()))\n",
    "\n",
    "'''\n",
    "Scores: accuracy is [0.56291391 0.54966887 0.37086093 0.2384106  0.38      ], mean accuary is 0.42037086092715226\n",
    "Scores: precision is [0.1509434  0.24675325 0.17       0.16793893 0.20183486], mean precision is 0.18749408733253856\n",
    "Scores: recall is [0.27586207 0.65517241 0.5862069  0.78571429 0.78571429], mean recall is 0.6177339901477832\n",
    "Scores: f1 is [0.19512195 0.35849057 0.26356589 0.27672956 0.32116788], mean f1 is 0.2830151703380446\n",
    "Scores: roc_auc is [0.4535048  0.58988129 0.45293951 0.44976771 0.53629977], mean roc_auc is 0.4964786170917511\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about dnn\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [
    {
     "data": {
      "text/plain": "(754, 66)"
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1.0)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    return epoch_loss, accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "# Training loop\n",
    "def train(model, criterion, optimizer, scheduler, train_dataloader, val_dataloader, device, num_epochs=10):\n",
    "    # best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        val_loss, accuracy, precision, recall, f1, roc_auc = evaluate(model, criterion, val_dataloader, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Training Loss: {epoch_loss:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, '\n",
    "              f'Precision: {precision:.4f}, '\n",
    "              f'Recall: {recall:.4f}, '\n",
    "              f'F1-score: {f1:.4f}, '\n",
    "              f'ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the minimum number of positive samples per split will reduce the performance\n",
    "\n",
    "X_dnn = torch.tensor(X).float()\n",
    "y_dnn = torch.tensor(y).float()\n",
    "# Split the data into training and validation sets\n",
    "dataset = TensorDataset(X_dnn, y_dnn)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = x.shape[1]\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = BinaryClassifier(input_size)\n",
    "# commonly use in binary classification\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=400, verbose=True)\n",
    "train(model, criterion, optimizer, scheduler, train_loader, val_loader, device, num_epochs=500)\n",
    "\n",
    "\n",
    "'''\n",
    "self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "drop 0.2 2 layers\n",
    "Training Loss: 0.0025, Validation Loss: 1.9067, Accuracy: 0.8013, Precision: 0.3000, Recall: 0.5000, F1-score: 0.3750, ROC AUC: 0.6711\n",
    "\n",
    "\n",
    "self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 1)\n",
    "Training Loss: 0.0005, Validation Loss: 1.3775, Accuracy: 0.8146, Precision: 0.3333, Recall: 0.5556, F1-score: 0.4167, ROC AUC: 0.7026\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}