{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# if using mac\n",
    "df = pd.read_excel('/Users/zhanghantang/PycharmProjects/sucide_project/BIOM40forUSC.xlsx')\n",
    "data = df[df['SI'].notnull()]\n",
    "y = data['SI']\n",
    "x = data.loc[:, 'GIMAP1Biom1552316_a_at':'CFIS']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603 151\n",
      "547 64 49 57\n"
     ]
    }
   ],
   "source": [
    "# column 'CFI-S.PheneVisit' data type is string, i am not clear its internal meaning and how to convert to float data type\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = x.drop(labels='CFI-S.PheneVisit', axis=1)\n",
    "# drop these column directly\n",
    "x = data.loc[:, 'GIMAP1Biom1552316_a_at':'RAB3GAP2Biom240234_at']\n",
    "y = np.array(y)\n",
    "X = StandardScaler().fit_transform(x)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "print(len(X_train), len(X_test))\n",
    "\n",
    "# todo unified randome state\n",
    "unified_random_state = 32\n",
    "y_list = list(y)\n",
    "print(y_list.count(0), y_list.count(1), y_list.count(2), y_list.count(3), y_list.count(4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "547"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about logistic regression content\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty:  l1\n",
      "Best C:  10\n",
      "Best score:  0.2006970876816408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', max_iter=5000, )\n",
    "# 不同的solver可能只支持不同的有限的penalty\n",
    "param_grid = {'penalty':['l1', 'l2'], 'C':[0.1, 1, 10, 100], 'multi_class':['ovr', 'multinomial']}\n",
    "\n",
    "# The macro-averaged F1 score gives equal weight to each class, best metric for our data\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro', return_train_score=True)\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best penalty: \", grid_search.best_params_['penalty'])\n",
    "print(\"Best C: \", grid_search.best_params_['C'])\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.64900662 0.68874172 0.63576159 0.69536424 0.64      ], mean accuary is 0.6617748344370861\n",
      "Scores: f1 is [0.18568306 0.16377953 0.18121804 0.220515   0.25228982], mean f1 is 0.2006970876816408\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nScores: accuracy is [0.64900662 0.68874172 0.63576159 0.69536424 0.64      ], mean accuary is 0.6617748344370861\\nScores: f1 is [0.18568306 0.16377953 0.18121804 0.220515   0.25228982], mean f1 is 0.2006970876816408\\n\\n\\n'"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "# scoring = ['accuracy', 'precision', 'recall', 'f1_macro', 'roc_auc']\n",
    "# scoring = {\n",
    "#     'accuracy': 'accuracy',\n",
    "#     'precision_macro':make_scorer(precision_score, zero_division=0, average='macro'),\n",
    "#     'recall_macro' :make_scorer(recall_score, average='macro'),\n",
    "#     'f1_macro':'f1_macro',\n",
    "#     'roc_auc':'roc_auc'\n",
    "# }\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro':'f1_macro',\n",
    "}\n",
    "lr_model = LR(solver='saga', penalty='l1', C=10, max_iter=5000, multi_class='multinomial')\n",
    "scores = cross_validate(lr_model, X, y, cv=5, scoring=scoring)\n",
    "# print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "# print(\"Scores: precision is {}, mean precision is {}\".format(scores['test_precision'], scores['test_precision'].mean()))\n",
    "# print(\"Scores: recall is {}, mean recall is {}\".format(scores['test_recall'], scores['test_recall'].mean()))\n",
    "# print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1'], scores['test_f1_macro'].mean()))\n",
    "# print(\"Scores: roc_auc is {}, mean roc_auc is {}\".format(scores['test_roc_auc'], scores['test_roc_auc'].mean()))\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1_macro'], scores['test_f1_macro'].mean()))\n",
    "\n",
    "\n",
    "'''\n",
    "Scores: accuracy is [0.64900662 0.68874172 0.63576159 0.69536424 0.64      ], mean accuary is 0.6617748344370861\n",
    "Scores: f1 is [0.18568306 0.16377953 0.18121804 0.220515   0.25228982], mean f1 is 0.2006970876816408\n",
    "\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about SVM\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  150\n",
      "Best score:  0.24237674223354908\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nBest penalty:  rbf\\nBest C:  100\\n\\nScores:  [0.36065574 0.44067797 0.45901639 0.31111111 0.40740741]\\nMean Score:  0.39577372315355086\\nScores:  [0.74274194 0.71833931 0.72649573 0.70439633 0.69226137]\\nMean Score:  0.7168469323092028\\n\\n'"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = SVC(kernel='rbf', max_iter=10000, class_weight='balanced')\n",
    "param_grid = {'C':[100, 150, 200], }\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro', return_train_score=True)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print(\"Best penalty: \", grid_search.best_params_['kernel'])\n",
    "print(\"Best C: \", grid_search.best_params_['C'])\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "'''\n",
    "Best penalty:  rbf\n",
    "Best C:  100\n",
    "\n",
    "Scores:  [0.36065574 0.44067797 0.45901639 0.31111111 0.40740741]\n",
    "Mean Score:  0.39577372315355086\n",
    "Scores:  [0.74274194 0.71833931 0.72649573 0.70439633 0.69226137]\n",
    "Mean Score:  0.7168469323092028\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.64900662 0.66225166 0.63576159 0.64900662 0.51333333], mean accuary is 0.6218719646799117\n",
      "Scores: f1 is [0.24741249 0.22642792 0.22244541 0.24700767 0.2695426 ], mean f1 is 0.2425672184240253\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nScores: accuracy is [0.64900662 0.66225166 0.63576159 0.64900662 0.50666667], mean accuary is 0.6205386313465784\\nScores: f1 is [0.24741249 0.22642792 0.22149303 0.24772155 0.26696575], mean f1 is 0.242004147773067\\n\\n'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel='rbf', max_iter=5000, class_weight='balanced', C=150)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro':'f1_macro',\n",
    "}\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=scoring)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1_macro'], scores['test_f1_macro'].mean()))\n",
    "\n",
    "'''\n",
    "Scores: accuracy is [0.64900662 0.66225166 0.63576159 0.64900662 0.50666667], mean accuary is 0.6205386313465784\n",
    "Scores: f1 is [0.24741249 0.22642792 0.22149303 0.24772155 0.26696575], mean f1 is 0.242004147773067\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about random forest\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 11, 'min_samples_leaf': 2, 'min_samples_split': 8, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "param_grid = {'n_estimators':[10, 100, 200], 'max_depth':[8, 11, 13], 'min_samples_split': [4, 6, 8, 10],\n",
    "              'min_samples_leaf': [2, 4, 6]}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "rf_model = RandomForestClassifier(random_state=unified_random_state)\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=skf, scoring='f1_macro')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.71523179 0.7218543  0.73509934 0.72847682 0.7       ], mean accuary is 0.7201324503311259\n",
      "Scores: f1 is [0.19153696 0.16833977 0.2006534  0.17054264 0.23872815], mean f1 is 0.19396018441926785\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nScores: accuracy is [0.7218543  0.7218543  0.73509934 0.72847682 0.70666667], mean accuary is 0.7227902869757175\\nScores: f1 is [0.16769231 0.16833977 0.19911406 0.16988417 0.25703768], mean f1 is 0.19241359726083926\\n\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro':'f1_macro',\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=10, max_depth=11, min_samples_leaf=2, min_samples_split=8, random_state=unified_random_state)\n",
    "\n",
    "# scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "scores = cross_validate(rf_model, X, y, cv=5, scoring=scoring)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1_macro'], scores['test_f1_macro'].mean()))\n",
    "\n",
    "# print(\"Score with L1 penalty: accuracy:{} precision:{} recall:{} f1:{} roc_auc:{}\".format(accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred), roc_auc_score(y_test, y_pred)))\n",
    "'''\n",
    "Scores: accuracy is [0.7218543  0.7218543  0.73509934 0.72847682 0.70666667], mean accuary is 0.7227902869757175\n",
    "Scores: f1 is [0.16769231 0.16833977 0.19911406 0.16988417 0.25703768], mean f1 is 0.19241359726083926\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "clf = XGBClassifier(random_state=unified_random_state, objective='multi:softmax', num_class=5, )\n",
    "param_grid = {'n_estimators':[10, 100, 200], 'max_depth':[4, 6, 8], 'learning_rate': [0.3, 0.01]}\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.62913907 0.68874172 0.73509934 0.69536424 0.61333333], mean accuary is 0.672335540838852\n",
      "Scores: f1 is [0.17405797 0.22587977 0.27777778 0.22605263 0.22640889], mean f1 is 0.22603540619990542\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nScores: accuracy is [0.69536424 0.70198675 0.7218543  0.71523179 0.62      ], mean accuary is 0.6908874172185431\\nScores: f1 is [0.24137887 0.22710069 0.24285714 0.22783049 0.2089161 ], mean f1 is 0.22961665754520685\\n'"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro':'f1_macro',\n",
    "}\n",
    "\n",
    "clf = XGBClassifier(n_estimators=100, learning_rate=0.01, max_depth=8, subsample=1)\n",
    "scores = cross_validate(clf, X, y, cv=5, scoring=scoring)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1_macro'], scores['test_f1_macro'].mean()))\n",
    "\n",
    "'''\n",
    "Scores: accuracy is [0.69536424 0.70198675 0.7218543  0.71523179 0.62      ], mean accuary is 0.6908874172185431\n",
    "Scores: f1 is [0.24137887 0.22710069 0.24285714 0.22783049 0.2089161 ], mean f1 is 0.22961665754520685\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about native bayes\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: accuracy is [0.2781457  0.21854305 0.15231788 0.11258278 0.16      ], mean accuary is 0.184317880794702\n",
      "Scores: f1 is [0.19394946 0.12108522 0.13812573 0.08840049 0.1179354 ], mean f1 is 0.1318992599341135\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nScores: accuracy is [0.56291391 0.54966887 0.37086093 0.2384106  0.38      ], mean accuary is 0.42037086092715226\\nScores: precision is [0.1509434  0.24675325 0.17       0.16793893 0.20183486], mean precision is 0.18749408733253856\\nScores: recall is [0.27586207 0.65517241 0.5862069  0.78571429 0.78571429], mean recall is 0.6177339901477832\\nScores: f1 is [0.19512195 0.35849057 0.26356589 0.27672956 0.32116788], mean f1 is 0.2830151703380446\\nScores: roc_auc is [0.4535048  0.58988129 0.45293951 0.44976771 0.53629977], mean roc_auc is 0.4964786170917511\\n\\n'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "nb = GaussianNB()\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro':'f1_macro',\n",
    "}\n",
    "\n",
    "scores = cross_validate(nb, X, y, cv=5, scoring=scoring)\n",
    "print(\"Scores: accuracy is {}, mean accuary is {}\".format(scores['test_accuracy'], scores['test_accuracy'].mean()))\n",
    "print(\"Scores: f1 is {}, mean f1 is {}\".format(scores['test_f1_macro'], scores['test_f1_macro'].mean()))\n",
    "\n",
    "'''\n",
    "Scores: accuracy is [0.56291391 0.54966887 0.37086093 0.2384106  0.38      ], mean accuary is 0.42037086092715226\n",
    "Scores: precision is [0.1509434  0.24675325 0.17       0.16793893 0.20183486], mean precision is 0.18749408733253856\n",
    "Scores: recall is [0.27586207 0.65517241 0.5862069  0.78571429 0.78571429], mean recall is 0.6177339901477832\n",
    "Scores: f1 is [0.19512195 0.35849057 0.26356589 0.27672956 0.32116788], mean f1 is 0.2830151703380446\n",
    "Scores: roc_auc is [0.4535048  0.58988129 0.45293951 0.44976771 0.53629977], mean roc_auc is 0.4964786170917511\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------\n",
    "the content below is about dnn\n",
    "-------------------------\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1.0, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "\n",
    "    return epoch_loss, accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "# Training loop\n",
    "def train(model, criterion, optimizer, scheduler, train_dataloader, val_dataloader, device, num_epochs=10):\n",
    "    # best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        val_loss, accuracy, precision, recall, f1, roc_auc = evaluate(model, criterion, val_dataloader, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Training Loss: {epoch_loss:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, '\n",
    "              f'Precision: {precision:.4f}, '\n",
    "              f'Recall: {recall:.4f}, '\n",
    "              f'F1-score: {f1:.4f}, '\n",
    "              f'ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Set the minimum number of positive samples per split will reduce the performance\n",
    "\n",
    "X_dnn = torch.tensor(X).float()\n",
    "y_dnn = torch.tensor(y).float()\n",
    "# Split the data into training and validation sets\n",
    "dataset = TensorDataset(X_dnn, y_dnn)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0538,  0.1139, -0.0963,  0.0773,  0.0005],\n",
      "        [-0.0863,  0.1259, -0.0721,  0.0988, -0.0041],\n",
      "        [-0.0732,  0.1252, -0.0851,  0.0873, -0.0095],\n",
      "        [-0.0584,  0.1138, -0.0897,  0.0835, -0.0038],\n",
      "        [-0.0603,  0.1211, -0.0931,  0.0803, -0.0017]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([3., 0., 0., 3., 1.])\n",
      "tensor(1.5979, grad_fn=<NllLossBackward0>)\n",
      "tensor([1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "test = X_dnn[0:5]\n",
    "input_size = x.shape[1]\n",
    "net = BinaryClassifier(input_size)\n",
    "outputs = net(test)\n",
    "labels = y_dnn[0:5].long()\n",
    "print(outputs)\n",
    "print(y_dnn[0:5])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(outputs, labels)\n",
    "print(loss)\n",
    "preds = torch.argmax(outputs, dim=1)\n",
    "print(preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = x.shape[1]\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.out = nn.Linear(32, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        out = self.out(x)\n",
    "        return out\n",
    "\n",
    "model = BinaryClassifier(input_size)\n",
    "# commonly use in binary classification\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=400, verbose=True)\n",
    "train(model, criterion, optimizer, scheduler, train_loader, val_loader, device, num_epochs=500)\n",
    "\n",
    "\n",
    "'''\n",
    "self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "drop 0.2 2 layers\n",
    "Training Loss: 0.0025, Validation Loss: 1.9067, Accuracy: 0.8013, Precision: 0.3000, Recall: 0.5000, F1-score: 0.3750, ROC AUC: 0.6711\n",
    "\n",
    "\n",
    "self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 1)\n",
    "Training Loss: 0.0005, Validation Loss: 1.3775, Accuracy: 0.8146, Precision: 0.3333, Recall: 0.5556, F1-score: 0.4167, ROC AUC: 0.7026\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}